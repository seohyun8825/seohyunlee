#!/usr/bin/env python3
"""
Manual crawl of specific Tistory posts based on web search results
"""

import requests
from bs4 import BeautifulSoup
import json
import re
import os
from datetime import datetime

class ManualTistoryCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def parse_korean_date(self, date_text):
        """Parse Korean date format to YYYY-MM-DD"""
        try:
            # Remove Korean text and extract date
            date_match = re.search(r'(\d{4})\.(\d{1,2})\.(\d{1,2})', date_text)
            if date_match:
                year, month, day = date_match.groups()
                return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        except:
            pass
        
        # Fallback to current date
        return datetime.now().strftime('%Y-%m-%d')
    
    def crawl_post(self, url, title, date, category, tags=None):
        """Crawl a specific post"""
        try:
            response = self.session.get(url)
            response.raise_for_status()
            html = response.text
            
            soup = BeautifulSoup(html, 'html.parser')
            
            # Extract content - try different selectors
            content = ""
            content_selectors = [
                'div.entry-content',
                'div.post-content', 
                'div.content',
                'article',
                'div.tt_article_useless_p_margin'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # Remove script and style tags
                    for script in content_elem(["script", "style"]):
                        script.decompose()
                    content = content_elem.get_text().strip()
                    break
            
            if not content:
                # Fallback: get all text from body
                body = soup.find('body')
                if body:
                    for script in body(["script", "style", "nav", "footer", "header"]):
                        script.decompose()
                    content = body.get_text().strip()
            
            return {
                'title': title,
                'date': date,
                'category': category,
                'content': content,
                'tags': tags or [],
                'url': url,
                'html': html
            }
            
        except Exception as e:
            print(f"Error crawling {url}: {e}")
            return None

def main():
    # Based on web search results, create posts manually
    posts_data = [
        {
            'title': 'Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware',
            'date': '2025-09-15',
            'category': 'Robot',
            'tags': ['ACT', 'Imitation Learning', 'Robotics', 'CVAE'],
            'content': '''ACT, Architecture of Action Chunking with Transformers
ACTÎûÄ, imitation learning ÏïåÍ≥†Î¶¨Ï¶òÏúºÎ°ú fine grainÌïòÍ≤å Î°úÎ¥áÏùÑ Ï°∞Ï¢ÖÌïòÍ∏∞ ÏúÑÌïú ÏïåÍ≥†Î¶¨Ï¶òÏù¥Îã§. ÏßëÏóê Î°úÎ¥áÏùÑ Ï°∞Î¶ΩÌïú Í∏∞ÎÖêÏúºÎ°ú ÏùΩÏñ¥Î¥§Îã§. Ï≤òÏùå Ï†ëÌïòÎäî Î∂ÑÏïºÎùº Î≠êÎ∂ÄÌÑ∞ ÏùΩÏñ¥ÏïºÌï†ÏßÄ Î™®Î•¥Í≤†Ïñ¥ÏÑú Ï∂îÏ≤úÏùÑ Î∞õÏïÑÏôîÎã§. ÏµúÍ∑ºÍπåÏßÄÎèÑ Í∞ÄÏû• ÎßéÏù¥ Ïì∞Ïù¥Îäî Î∞©ÏãùÏù¥ÎùºÍ≥†ÌïúÎã§.

Ï∞∏ Ìù•ÎØ∏Î°úÏö¥ Î°úÎ¥áÏÑ∏ÏÉÅÏù¥Îã§ ÎäêÎ¶¨ÏßÄÎßå RTX 4050ÏóêÏÑú vla ÌïôÏäµÏù¥Îûë inference Îã§ ÏûòÎèåÏïÑÍ∞ÑÎã§ single stepÏùÑ predictÌïòÎäî ÎåÄÏã†Ïóê chunks of actionÏùÑ ÏòàÏ∏°ÌïúÎã§ÎäîÍ≤å ÌïµÏã¨ ÏïÑÏù¥ÎîîÏñ¥Ïù¥Îã§. Conditional Variational Autoenncoder , CVAEÎ•º transformerÎ°ú ÎßåÎì† Íµ¨Ï°∞Ïù¥Îã§.

ÌòÑÏû¨ joint spaceÏóêÏÑú actionÏùÑ ÏòàÏ∏°ÌïòÎäîÎç∞, Ïù¥Í≤å Î°úÎ¥áÏùò Î¨ºÎ¶¨Ï†Å Ï†úÏïΩÍ≥º ÎßûÏßÄ ÏïäÏùÑ Ïàò ÏûàÎã§. Í∑∏ÎûòÏÑú ACTÎäî action chunkingÏùÑ ÌÜµÌï¥ Îçî ÏûêÏó∞Ïä§Îü¨Ïö¥ Î°úÎ¥á Ï†úÏñ¥Î•º Í∞ÄÎä•ÌïòÍ≤å ÌïúÎã§.'''
        },
        {
            'title': 'Concept-skill Transferability-based Data Selection for Large Vision-Language Models',
            'date': '2025-08-16',
            'category': 'Computer Vision',
            'tags': ['VLM', 'Data Selection', 'Clustering', 'Training Efficiency'],
            'content': '''Intro
Îç∞Ïù¥ÌÑ∞ÏÖãÏù¥ ÎÑàÎ¨¥ ÌÅ∞ Í≤ΩÏö∞ ÌïôÏäµ ÎπÑÏö©Ïù¥ ÎÑàÎ¨¥ ÎßéÏù¥ Îì†Îã§. Í∑∏ÎûòÏÑú Ï¢ãÏùÄ Îç∞Ïù¥ÌÑ∞Îßå Ïñ¥ÎñªÍ≤å Ïûò ÎΩëÏïÑ Ïì∏Íπå, Ïóê ÎåÄÌïú Ïó∞Íµ¨Ïù¥Îã§.

ÌïµÏã¨ ÏïÑÏù¥ÎîîÏñ¥Îäî ÏûëÏùÄ vlm ÏùÑ ÏÇ¨Ïö©Ìï¥ÏÑú, Ïñ¥Îñ§ Îç∞Ïù¥ÌÑ∞Í∞Ä Ïú†Ïö©Ìï†ÏßÄ Í≥®ÎùºÎÇ∏Îã§. ÏûëÏùÄ Î™®Îç∏Ïùò ÎÇ¥Î∂Ä representationÏùÑ Ïù¥Ïö©Ìï¥ÏÑú training Îç∞Ïù¥ÌÑ∞Î•º ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅÏùÑ ÌïúÎã§. Í∑∏ÎûòÏÑú Îç∞Ïù¥ÌÑ∞ ÏÜçÏóêÏÑú concept - skill compositionÏùÑ ÌååÏïÖÌïúÎã§. (street sign/OCR) Ïù¥Îü∞ÏãùÏúºÎ°ú Ïñ¥Îñ§ Ïª®ÏÖâÏù∏ÏßÄ, Í∑∏Î¶¨Í≥† Ïù¥Í±∏ ÎãµÌïòÍ∏∞ ÏúÑÌï¥ÏÑúÎäî Ïñ¥Îñ§ skill Ïù¥ ÌïÑÏöîÌïúÏßÄ Ïù¥Îü∞ÏãùÏúºÎ°ú Ï°∞Ìï©ÌïòÎäî ÎäêÎÇåÏù∏Í±∞Í∞ôÎã§.

Ïù¥Î†áÍ≤å ÌïòÎ©¥ Îã§ÏñëÌïú Îç∞Ïù¥ÌÑ∞Î•º ÏÜåÎüâÎßå ÎΩëÏïÑÎèÑ ÏÑ±Îä• Ïú†ÏßÄÍ∞Ä Í∞ÄÎä•ÌïòÎã§. Í∑∏ÎûòÏÑú concept-skill compositionÏúºÎ°ú ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅÏùÑ Ìï¥ÎÜìÍ≥†, Í∞Å ÌÅ¥Îü¨Ïä§ÌÑ∞ÏóêÏÑú densityÎ•º Í≥ÑÏÇ∞Ìï¥ÏÑú ÎåÄÌëúÏ†ÅÏù∏ ÏÉòÌîåÎì§ÏùÑ ÏÑ†ÌÉùÌïúÎã§.'''
        },
        {
            'title': 'Log-Linear Attention Îπ†Î•∏ Ìä∏ÎûúÏä§Ìè¨Î®∏!?',
            'date': '2025-06-27',
            'category': 'Computer Vision',
            'tags': ['Transformer', 'Attention', 'Long Context', 'Efficiency'],
            'content': '''long contextÎ•º Ïñ¥ÎñªÍ≤å Ï≤òÎ¶¨Ìï†ÏßÄ, AIÏóêÏÑúÎäî ÍµâÏû•Ìûà Ï§ëÏöîÌïú Î¨∏Ï†úÏù¥Îã§. Transformer,, ÍµâÏû•ÌïòÏßÄÎßå Îã®Ï†êÏù¥ ÎÑàÎ¨¥ÎÇòÎèÑ Î™ÖÌôïÌïòÎã§. Î∞îÎ°ú quadratic-compute Ïù¥Îã§.

Ïñ¥ÎîîÏÑú bottleneckÏù¥ Î∞úÏÉùÌïòÎÉêÎ©¥, transformerÏùò selfattention Í∑∏Î¶ºÏù∏Îç∞, attention matrixÎ∂ÄÎ∂ÑÏùÑ Î≥¥Ïûê. attention matrixÏù¥ n^2 Ïù¥ ÎêòÎäîÎç∞, Í∑∏Îüº number of tokenÏù¥ ÎßéÏïÑÏßÄÎ©¥ n^2Î∞∞ÎßåÌÅº quadraticÌïòÍ≤å ÎäòÏñ¥ÎÇòÍ≤å Îê†Í≤ÉÏù¥Îã§.

Log-Linear AttentionÏùÄ Ïù¥ Î¨∏Ï†úÎ•º Ìï¥Í≤∞ÌïòÍ∏∞ ÏúÑÌï¥ attention Í≥ÑÏÇ∞ÏùÑ log-linear complexityÎ°ú Ï§ÑÏù¥Îäî Î∞©Î≤ïÏù¥Îã§. Í∏∞Ï°¥Ïùò softmax attention ÎåÄÏã† log-spaceÏóêÏÑú Í≥ÑÏÇ∞ÏùÑ ÏàòÌñâÌïòÏó¨ Î©îÎ™®Î¶¨ÏôÄ Í≥ÑÏÇ∞ Ìö®Ïú®ÏÑ±ÏùÑ ÌÅ¨Í≤å Ìñ•ÏÉÅÏãúÌÇ®Îã§.'''
        },
        {
            'title': 'ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in Hour-Long Videos',
            'date': '2025-05-31',
            'category': 'Computer Vision',
            'tags': ['VLM', 'Video Understanding', 'Temporal Grounding', 'Recursive'],
            'content': '''Í∏¥ ÏòÅÏÉÅÏóêÏÑú 'Ïñ∏Ï†ú, Ïñ¥Îñ§ ÏùºÏù¥ ÏùºÏñ¥ÎÇ¨ÎäîÏßÄ'Î•º Ï∞æÏïÑÎÇ¥Îäî Í≤ÉÏùÑ Í∏∞Ï°¥ VLMÏùÄ Ïûò Î™ªÌïúÎã§. ÌîÑÎ†àÏûÑ Ïàò ÌïúÍ≥Ñ ÎïåÎ¨∏Ïóê Ï§ëÏöîÌïú ÏàúÍ∞ÑÏù¥ ÎàÑÎùΩÎêòÍ∏∞ ÏâΩÍ≥†, Í≤∞Í≥ºÏ†ÅÏúºÎ°ú ÏãúÍ∞Ñ Í≤ΩÍ≥ÑÍ∞Ä ÌùêÎ¶øÌï¥ÏßÑÎã§.

ReVisionLLMÏùÄ Ïù¥ ÌïúÍ≥ÑÎ•º Í∑πÎ≥µÌïòÍ∏∞ ÏúÑÌï¥ Ïù∏Í∞ÑÏù¥ ÏòÅÏÉÅÏùÑ ÌõëÎäî Î∞©ÏãùÏùÑ Î™®Î∞©Ìï¥ÏÑú Ïû¨Í∑ÄÏ†Å ÌÉêÏÉâ Î∞©ÏãùÏùÑ ÏÇ¨Ïö©ÌïúÎã§. Î™®Îç∏ÏùÄ Î®ºÏ†Ä Ï†ÄÌï¥ÏÉÅÎèÑ Ï†ÑÏ≤¥ scanÏúºÎ°ú Í¥ÄÏã¨ Íµ¨Í∞ÑÏùÑ ÎåÄÎûµÏ†ÅÏúºÎ°ú ÏßÄÏ†ïÌïúÎã§. Ïù¥ÌõÑ Ìï¥Îãπ Íµ¨Í∞ÑÎßå ÌîÑÎ†àÏûÑ Ìï¥ÏÉÅÎèÑÎ•º ÎÜíÏó¨ Îã§Ïãú Î∂ÑÏÑùÌïòÍ≥†, ÌïÑÏöîÌïòÎ©¥ Îçî ÏÑ∏Î∞ÄÌïòÍ≤å ÌôïÎåÄÌïúÎã§.

Ïù¥Î†áÍ≤å "ÎÑìÍ≤å ‚Üí Ï¢ÅÍ≤å ‚Üí Îçî Ï¢ÅÍ≤å" Í≥ºÏ†ïÏùÑ Î∞òÎ≥µÌïòÎ©∞ ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú Ï¥à Îã®ÏúÑ Í≤ΩÍ≥ÑÎ•º ÏÇ∞Ï∂úÌïúÎã§. ÌõàÎ†® Í≥ºÏ†ïÎèÑ hierarchical ÌïòÍ≤å ÏÑ§Í≥ÑÌïúÎã§. ÏßßÏùÄ 10‚Äì30 Ï¥à ÌÅ¥Î¶ΩÏóê Î®ºÏ†Ä ÏÇ¨Í±¥ Ïù∏ÏßÄ Îä•Î†•ÏùÑ ÌïôÏäµÏãúÌÇ® Îí§, Ï†êÏ∞® Í∏∏Ïù¥Î•º ÎäòÎ†§ Î™á ÏãúÍ∞ÑÏßúÎ¶¨ ÏòÅÏÉÅÍπåÏßÄ ÌôïÏû• ÌïôÏäµÏùÑ ÏßÑÌñâÌïúÎã§.'''
        },
        {
            'title': 'DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models',
            'date': '2025-04-27',
            'category': 'NLP',
            'tags': ['Math', 'Reasoning', 'Data Quality', 'GRPO'],
            'content': '''Ï°∞Í∏à Ïò§ÎûòÎêú.. ÏûëÎÖÑ Ï¥à ÎÖºÎ¨∏Ïù¥Í∏¥ ÌïòÏßÄÎßå, ÌäπÏ†ï Task ÏóêÏÑú SotaÎ•º Îã¨ÏÑ±ÌïòÍ∏∞ ÏúÑÌï¥ Ïñ¥Îñ§ Î∞©Î≤ïÏùÑ ÏÇ¨Ïö©ÌñàÍ≥†, ÌäπÌûà Î∂ÄÏ°±Ìïú Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ Ïñ¥ÎñªÍ≤å Ìï¥Í≤∞ÌñàÎäîÏßÄ Í∂ÅÍ∏àÌï¥ÏÑú ÏùΩÏñ¥Î¥§Îã§.

Ïö∞ÏÑ† DeepSeekMathÎäî ÏàòÌïô Î≤§ÏπòÎßàÌÅ¨ÏóêÏÑú ÎãπÏãú SotaÎ•º Îã¨ÏÑ±ÌñàÎã§. ÎëêÍ∞ÄÏßÄ Ï†ëÍ∑ºÎ≤ïÏù¥ ÏûàÏóàÎäîÎç∞ ÏïÑÏ£ºÏïÑÏ£º ÎßéÏùÄ Í≥†ÌíàÏßàÏùò Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Î™®ÏùÄÍ≤É, Í∑∏Î¶¨Í≥† GRPO Ïù¥Îã§.

Î®ºÏ†Ä ÏïÑÏ£ºÏïÑÏ£º ÎßéÏùÄ Í≥†ÌíàÏßàÏùò Îç∞Ïù¥ÌÑ∞ÏÖãÏùÄ Ïñ¥ÎñªÍ≤å Î™®ÏïòÏùÑÍπå. ÏïåÎã§ÏãúÌîº Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ ÎßåÎìúÎäîÍ±¥ ÎèàÏù¥ ÎßéÏù¥ Îì†Îã§. GPTÎ•º ÌÉúÏõåÏÑú Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ ÎßåÎì§Îã§Î≥¥Î©¥ ÏàúÏãùÍ∞ÑÏóê Î™áÎ∞±Ïù¥ ÎÇòÍ∞ÄÏûàÏùÑÍ±∞Îã§.. Ïó¨Í∏∞ÏÑú ÏÜåÍ∞úÌïú Îç∞Ïù¥ÌÑ∞ÏÖã Î™®ÏúºÍ∏∞ ÌååÏù¥ÌîÑÎùºÏù∏Ïùò Ï£ºÏöî ÏïÑÏù¥ÎîîÏñ¥Îäî, Îç∞Ïù¥ÌÑ∞Î•º Ï†úÏûëÌïòÎäîÍ≤å ÏïÑÎãàÍ≥†, Ïù¥ÎØ∏ ÏûàÎäî Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Ïûò Í±∏Îü¨ÎÇ¥ÏÑú ÌïÑÏöîÌïúÍ≤ÉÎì§ÏùÑ Ïûò ÎΩëÏïÑÎÇ¥Î≥¥Ïûê Ïù¥Îã§.'''
        }
    ]
    
    # Convert to blog format
    blog_data = {
        "posts": [],
        "categories": {},
        "tags": {}
    }
    
    for i, post_data in enumerate(posts_data):
        post_id = f"post-{i+1}"
        
        # Clean content for excerpt
        content = post_data['content']
        excerpt = content[:200] + "..." if len(content) > 200 else content
        
        blog_post = {
            "id": post_id,
            "title": post_data['title'],
            "date": post_data['date'],
            "category": post_data['category'],
            "tags": post_data['tags'],
            "excerpt": excerpt,
            "content": content,
            "original_url": f"https://happy8825.tistory.com/entry/{i+1}"
        }
        
        blog_data["posts"].append(blog_post)
        
        # Count categories
        category = post_data['category']
        if category not in blog_data["categories"]:
            blog_data["categories"][category] = 0
        blog_data["categories"][category] += 1
        
        # Count tags
        for tag in post_data['tags']:
            if tag not in blog_data["tags"]:
                blog_data["tags"][tag] = 0
            blog_data["tags"][tag] += 1
    
    # Convert counts to arrays for frontend
    blog_data["categories"] = [
        {"name": name, "count": count, "color": "#667eea"} 
        for name, count in blog_data["categories"].items()
    ]
    blog_data["tags"] = [
        {"name": name, "count": count} 
        for name, count in blog_data["tags"].items()
    ]
    
    # Save to file
    os.makedirs("pages/blog", exist_ok=True)
    with open("pages/blog/posts.json", 'w', encoding='utf-8') as f:
        json.dump(blog_data, f, ensure_ascii=False, indent=2)
    
    print(f"Created {len(posts_data)} posts from manual data")
    print("Saved to pages/blog/posts.json")
    
    # Create individual HTML files
    os.makedirs("pages/blog/posts", exist_ok=True)
    
    for post_data in posts_data:
        # Create safe filename
        safe_title = re.sub(r'[^\w\s-]', '', post_data['title'])
        safe_title = re.sub(r'[-\s]+', '-', safe_title)
        filename = f"{post_data['date']}-{safe_title}.html"
        
        # Create HTML content
        back_arrow = "‚Üê"
        
        category_html = f"<span> | üè∑Ô∏è {post_data['category']}</span>" if post_data['category'] else ""
        
        tags_html = ""
        if post_data['tags']:
            tag_spans = [f'<span style="background: rgba(255,255,255,0.1); padding: 0.25rem 0.5rem; border-radius: 12px; margin-right: 0.5rem; font-size: 0.9rem;">#{tag}</span>' for tag in post_data['tags']]
            tags_html = f"<div class='tags'>{''.join(tag_spans)}</div>"
        
        html_content = f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{post_data['title']} - Seohyun Lee Blog</title>
    <meta name="description" content="{excerpt[:160]}">
    <link rel="stylesheet" href="../../styles.css">
    <style>
        body {{ 
            background: #0d0f17; 
            color: #fff; 
            font-family: 'Inter', sans-serif; 
            line-height: 1.6; 
            margin: 0; 
            padding: 2rem;
            max-width: 800px;
            margin: 0 auto;
        }}
        .post-header {{
            border-bottom: 1px solid rgba(255,255,255,0.1);
            padding-bottom: 2rem;
            margin-bottom: 2rem;
        }}
        .post-meta {{
            color: #888;
            margin-bottom: 1rem;
        }}
        .post-title {{
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: 1rem;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }}
        .post-content {{
            font-size: 1.1rem;
            line-height: 1.8;
            white-space: pre-wrap;
        }}
        .back-link {{
            display: inline-block;
            margin-bottom: 2rem;
            color: #667eea;
            text-decoration: none;
            padding: 0.5rem 1rem;
            border: 1px solid rgba(102, 126, 234, 0.3);
            border-radius: 8px;
            transition: all 0.3s ease;
        }}
        .back-link:hover {{
            background: rgba(102, 126, 234, 0.2);
        }}
    </style>
</head>
<body>
    <a href="../blog.html" class="back-link">{back_arrow} Back to Blog</a>
    
    <div class="post-header">
        <div class="post-meta">
            <span>üìÖ {post_data['date']}</span>
            {category_html}
            <span> | üîó <a href="https://happy8825.tistory.com" target="_blank">Original Tistory</a></span>
        </div>
        <h1 class="post-title">{post_data['title']}</h1>
        {tags_html}
    </div>
    
    <div class="post-content">
{post_data['content']}
    </div>
    
    <div style="margin-top: 3rem; padding-top: 2rem; border-top: 1px solid rgba(255,255,255,0.1); text-align: center;">
        <a href="../blog.html" class="back-link">{back_arrow} Back to Blog</a>
    </div>
</body>
</html>"""
        
        # Save file
        filepath = os.path.join("pages/blog/posts", filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"Created: {filename}")

if __name__ == "__main__":
    main()
